{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82272dfa-1888-4c9a-9256-3d2622684726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ultilize the accelerate for academic resource\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82424a16-b7b1-46a6-8857-f5b25bce15f0",
   "metadata": {},
   "source": [
    "# Total Pipeline\n",
    "## 1. Data Preprocess\n",
    "### 1.1 Set Configs\n",
    "To manage the configurations effectively, I've implemented the `Config` class in utils/load_configs.py. This class is capable of loading configurations from JSON files and can be utilized in various scenarios. In essence, it simplifies the input parameters for functions and classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a22d9e5b-84d4-49a5-9f00-584730b99017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.load_configs import Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a85985-3b30-4b05-b7df-d4fd20f194c4",
   "metadata": {},
   "source": [
    "Then, we have some JSON files located in the config_files directory, each containing pre-set parameters for different models and stages. Next, we'll load all of these JSON files. The specifics will be explained later when we utilize these configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e16aca4-8816-4163-a6cc-606458b6cc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corresponding to stage1 method 1 mentioned below.\n",
    "stage1_train_config_using_lavis = Config()\n",
    "stage1_train_config_using_lavis.stage1_read_from_json(\"config_files/stage1_train_configs_lavis.json\")\n",
    "# corresponding to stage1 method 2 mentioned below.\n",
    "stage1_train_config_seperatly_extract = Config()\n",
    "stage1_train_config_seperatly_extract.stage1_read_from_json(\"config_files/stage1_train_configs.json\")\n",
    "\n",
    "# corresponding to stage1 method 1 mentioned below.\n",
    "stage1_dev_config_using_lavis = Config()\n",
    "stage1_dev_config_using_lavis.stage1_read_from_json(\"config_files/stage1_dev_configs_lavis.json\")\n",
    "# corresponding to stage1 method 2 mentioned below.\n",
    "stage1_dev_config_seperatly_extract = Config()\n",
    "stage1_dev_config_seperatly_extract.stage1_read_from_json(\"config_files/stage1_dev_configs.json\")\n",
    "\n",
    "# corresponding to stage1 method 1 mentioned below.\n",
    "stage1_test_config_using_lavis = Config()\n",
    "stage1_test_config_using_lavis.stage1_read_from_json(\"config_files/stage1_test_configs_lavis.json\")\n",
    "# corresponding to stage1 method 2 mentioned below.\n",
    "stage1_test_config_seperatly_extract = Config()\n",
    "stage1_test_config_seperatly_extract.stage1_read_from_json(\"config_files/stage1_test_configs.json\")\n",
    "\n",
    "# corresponding to stage2 method 1 menthioned below.\n",
    "stage2_train_config = Config()\n",
    "stage2_train_config.stage2_read_from_json(\"config_files/stage2_train_configs.json\")\n",
    "\n",
    "stage2_dev_config = Config()\n",
    "stage2_dev_config.stage2_read_from_json(\"config_files/stage2_dev_configs.json\")\n",
    "\n",
    "stage2_test_config = Config()\n",
    "stage2_test_config.stage2_read_from_json(\"config_files/stage2_test_configs.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032cf6d1-5627-4835-9a61-12c4147552a2",
   "metadata": {},
   "source": [
    "### 1.2 load news data\n",
    "In my project, the data loading process has been extensively wrapped, allowing for the use of a simple function to accomplish it.<br>\n",
    "\n",
    "You can directly call `Get_news_data(configs)` from utils/load_from_files.py to retrieve the news data. This process will take some time to load the data from the files, approximately 2 minutes in my environment using **72GB memory and 32GB V100**. It utilizes the function `extract_path_from_config(configs)` from the same file as `Get_news_data(configs)` to parse the image folder and the path to the news data."
   ]
  },
  {
   "cell_type": "raw",
   "id": "3582b7ad-5010-4249-a6b9-f8df034e34b6",
   "metadata": {},
   "source": [
    "from utils.load_from_files import Get_news_data\n",
    "\n",
    "# now there is no difference between seperatly extract and the lavis.\n",
    "train_news_data = Get_news_data(stage1_train_config_seperatly_extract)\n",
    "# Out of the consideration on memory, we will run it seperately.\n",
    "# dev_news_data = Get_news_data(stage1_dev_config_seperatly_extract)\n",
    "# test_news_data = Get_news_data(stage1_test_config_seperatly_extract)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4fbe0e-76eb-4a6b-b005-a4f2dc36213f",
   "metadata": {},
   "source": [
    "After loading all the news data, we should organize it into a dataset inherited from the PyTorch dataset. You can refer to the code in the data_processor folder for details about the dataset structure, and the README.md file explains how it functions.<br>\n",
    "\n",
    "The method `get_NewsDataset(configs)` is a wrapper function to obtain a news dataset using the provided configuration, thus avoiding the need to explicitly parse the parameters.<br>\n",
    "\n",
    "Now that we've acquired the news data, in order to reduce memory usage, we'll simply initialize the NewsDataset using the standard method."
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a0b9973-1867-44fa-939c-8532a4751039",
   "metadata": {},
   "source": [
    "from data_processor.NewsDataset import get_NewsDataset,NewsDataset\n",
    "\n",
    "# news_dataset = get_NewsDataset(stage1_config_seperatly_extract)\n",
    "# if using the function above, it will load the news data again.\n",
    "train_news_dataset = NewsDataset(train_news_data)\n",
    "print(\"the length of train news dataset is:\",len(train_news_dataset))\n",
    "\n",
    "# when process the dev and test dataset, the function is same with train dataset.\n",
    "# dev_news_dataset = NewsDataset(dev_news_data)\n",
    "# print(\"the length of dev news dataset is:\",len(dev_news_dataset))\n",
    "# test_news_dataset = NewsDataset(test_news_data)\n",
    "# print(\"the length of test news dataset is:\",len(test_news_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c167a280-7a2f-47a2-9e44-01ce4a689eb2",
   "metadata": {},
   "source": [
    "The each item in dataset includes: (newsid, title, abstract, img)<br>\n",
    "\n",
    "Then, we should load a Dataloader, so that it can collaborate with the other pytorch components. For future extensions, I've inherited the pytorch Dataloader as `BaseDataloader`, and then `NewsDataloader`, `UserDataloader` inherited from `BaseDataloader`, each without additional methods."
   ]
  },
  {
   "cell_type": "raw",
   "id": "94cebaaa-adcc-4b4d-bee6-02265f805153",
   "metadata": {},
   "source": [
    "from data_processor.NewsDataloader import NewsDataLoader\n",
    "\n",
    "train_news_dataloader = NewsDataLoader(train_news_dataset, stage1_train_config_seperatly_extract)\n",
    "\n",
    "# dev_news_dataloader = NewsDataLoader(dev_news_dataset, stage1_dev_config_seperatly_extract)\n",
    "# test_news_dataloader = NewsDataLoader(test_news_dataset, stage1_test_config_seperatly_extract)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7999cdff-311a-4f2e-b57d-e453bf2df739",
   "metadata": {},
   "source": [
    "After successfully constructing the `news_dataloader`, we can employ it for training or any other tasks within the PyTorch framework.<br>\n",
    "\n",
    "Although it won't be utilized in training due to efficiency considerations (matching each news and clicked user pair each epoch would consume a significant amount of time and memory), it can aid us in comprehending the `UserDataLoader`. <br>\n",
    "\n",
    "However, there is one noteworthy detail: the batch structure in `NewsDataloader` is transposed, with the batch size appearing in the second position. We can delve into this with the following code:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "05a2c91e-11bb-47b0-87d2-c79fa15208b5",
   "metadata": {},
   "source": [
    "for batch in train_news_dataloader:\n",
    "    print(type(batch[0]))\n",
    "    print(\"batch size is 128 but len(batch) is:\",len(batch))\n",
    "    print(\"len(batch[0]) is:\",len(batch[0]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a491a907-f6eb-40ac-88e6-341102dbfb52",
   "metadata": {},
   "source": [
    "### 1.3 Embedding\n",
    "Now that we have obtained the news data using the codes above, the next step is to embed the news into a 768-dimensional vector. I've devised two methods to investigate the effectiveness of multimodal pretraining:\n",
    "\n",
    "1. Utilizing the Lavis package, which offers the BLIP pretrained model to extract features.\n",
    "2. Employing the transformers package, which provides BCEmbedding and Dinov2-base for textual and visual extraction respectively.<br>\n",
    "\n",
    "In theory,  method one should outperform method two, and this will be validated in the subsequent experiments.<br>\n",
    "\n",
    "Now, we use the method 2 first. We can employ the `News_Embedding_Pipeline.save_encode(dataloader)` to achieve it, it is wrapped in the News_Embedding_Pipeline.py. When running, it will raise a future warning caused by `use_auth_token`, limited by the environment, it can't be solved. It will also take some time to calculate, I have set the tqdm to monitor progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f60e8b4f-1b32-46ad-9226-08069b4870c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/root/miniconda3/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:739: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "04/14/2024 23:06:04 - [INFO] - Text Feature Extracter ->>> Successfully load maidalun1020/bce-embedding-base_v1 from hugging face\n",
      "04/14/2024 23:06:07 - [INFO] - Text Feature Extracter ->>> Execute device: cuda;\t gpu num: 1;\t\n",
      "/root/miniconda3/lib/python3.8/site-packages/transformers/models/auto/image_processing_auto.py:344: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "04/14/2024 23:06:07 - [INFO] - Visual Feature Extractor ->>> Successfully load facebook/dinov2-base from hugging face\n",
      "04/14/2024 23:06:07 - [INFO] - Visual Feature Extractor ->>> Execute device: cuda;\t gpu num: 1;\t\n",
      "Saving Encode: 100%|██████████| 201/201 [06:35<00:00,  1.97s/it]\n"
     ]
    }
   ],
   "source": [
    "# method 2, using the pretrained textual model and visual model seperatly.\n",
    "from News_Embedding_Pipeline import News_Embedding_Pipeline\n",
    "from tqdm import tqdm\n",
    "train_news_embedder = News_Embedding_Pipeline(stage1_train_config_seperatly_extract)\n",
    "\n",
    "train_news_embedder.save_encode(train_news_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfe4f7b2-5f38-4f83-b01a-15a92efea252",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "get images of news now.: 100%|██████████| 42416/42416 [00:56<00:00, 751.89it/s]\n",
      "/root/miniconda3/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/root/miniconda3/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:739: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "04/14/2024 23:15:53 - [INFO] - Text Feature Extracter ->>> Successfully load maidalun1020/bce-embedding-base_v1 from hugging face\n",
      "04/14/2024 23:15:53 - [INFO] - Text Feature Extracter ->>> Execute device: cuda;\t gpu num: 1;\t\n",
      "/root/miniconda3/lib/python3.8/site-packages/transformers/models/auto/image_processing_auto.py:344: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "04/14/2024 23:15:54 - [INFO] - Visual Feature Extractor ->>> Successfully load facebook/dinov2-base from hugging face\n",
      "04/14/2024 23:15:55 - [INFO] - Visual Feature Extractor ->>> Execute device: cuda;\t gpu num: 1;\t\n",
      "Saving Encode: 100%|██████████| 166/166 [05:20<00:00,  1.93s/it]\n"
     ]
    }
   ],
   "source": [
    "# to avoid the memery overflow, the following code will be run seperately to get \n",
    "# news embedding title dev/test hadamard/lavis.\n",
    "from News_Embedding_Pipeline import News_Embedding_Pipeline\n",
    "from data_processor.NewsDataset import get_NewsDataset,NewsDataset\n",
    "from data_processor.NewsDataloader import NewsDataLoader\n",
    "from utils.load_from_files import Get_news_data\n",
    "\n",
    "dev_news_data = Get_news_data(stage1_dev_config_seperatly_extract)\n",
    "dev_news_dataset = NewsDataset(dev_news_data)\n",
    "dev_news_dataloader = NewsDataLoader(dev_news_dataset, stage1_dev_config_seperatly_extract)\n",
    "dev_news_embedder = News_Embedding_Pipeline(stage1_dev_config_seperatly_extract)\n",
    "dev_news_embedder.save_encode(dev_news_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840ff9f6-a445-417b-8f97-825426c02445",
   "metadata": {},
   "source": [
    "Due to there is no test dataset for MIND-Small, the code in the next section won't run. I just save the result of loading test dataset in MIND."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ac732d4-9e8b-4e47-955d-c08dc440c902",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "get images of news now.: 100%|██████████| 120959/120959 [02:46<00:00, 725.16it/s]\n",
      "/root/miniconda3/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/root/miniconda3/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:739: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "04/14/2024 15:46:32 - [INFO] - Text Feature Extracter ->>> Successfully load maidalun1020/bce-embedding-base_v1 from hugging face\n",
      "04/14/2024 15:46:34 - [INFO] - Text Feature Extracter ->>> Execute device: cuda;\t gpu num: 1;\t\n",
      "/root/miniconda3/lib/python3.8/site-packages/transformers/models/auto/image_processing_auto.py:344: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "04/14/2024 15:46:36 - [INFO] - Visual Feature Extractor ->>> Successfully load facebook/dinov2-base from hugging face\n",
      "04/14/2024 15:46:36 - [INFO] - Visual Feature Extractor ->>> Execute device: cuda;\t gpu num: 1;\t\n",
      "Saving Encode: 100%|██████████| 473/473 [15:09<00:00,  1.92s/it]\n"
     ]
    }
   ],
   "source": [
    "# to avoid the memery overflow, the following code will be run seperately to get \n",
    "# news embedding title dev/test hadamard/lavis.\n",
    "from News_Embedding_Pipeline import News_Embedding_Pipeline\n",
    "from data_processor.NewsDataset import get_NewsDataset,NewsDataset\n",
    "from data_processor.NewsDataloader import NewsDataLoader\n",
    "from utils.load_from_files import Get_news_data\n",
    "\n",
    "test_news_data = Get_news_data(stage1_test_config_seperatly_extract)\n",
    "test_news_dataset = NewsDataset(test_news_data)\n",
    "test_news_dataloader = NewsDataLoader(test_news_dataset, stage1_test_config_seperatly_extract)\n",
    "test_news_embedder = News_Embedding_Pipeline(stage1_test_config_seperatly_extract)\n",
    "test_news_embedder.save_encode(test_news_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3e53c9-8280-4012-bfd6-3d7dbc3fb41a",
   "metadata": {},
   "source": [
    "After embedding it, it will be saved at the path correlate with the `stage1_config_seperatly_extract.emb_path`.<br>\n",
    "\n",
    "Then, we can load it from file to get the news embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e812ac11-edde-4962-8db1-f4610acbfec1",
   "metadata": {},
   "source": [
    "### 1.4 load user data\n",
    "With the embedded news, we can match the user and the news embeddings respectively. It will cost a lot of time and memeory, thus it is necessary to save as a pkl file.<br>\n",
    "To achieve it, we need to load user data to `UserDataset` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2d51b2c-45fb-4606-9871-1eb77262d098",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/14/2024 23:41:46 - [INFO] - UserDataset ->>> successfully built raw user dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['U13740', ['N55189', 'N42782', 'N34694', 'N45794', 'N18445', 'N63302', 'N10414', 'N19347', 'N31801'], ['N55689-1', 'N35729-0']]\n"
     ]
    }
   ],
   "source": [
    "from data_processor.UserDataset import get_UserDataset,UserDataset\n",
    "\n",
    "raw_train_user_dataset = get_UserDataset(stage1_train_config_seperatly_extract)\n",
    "print(raw_train_user_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925dbc47-0fde-422d-a17b-267f0605d75d",
   "metadata": {},
   "source": [
    "Now, we can process the `raw_user_dataset`. It should be a list with shape: [batchsize, clicked_histories, 768].<br>\n",
    "\n",
    "But now, it is [batchsize, clicked_histories], we need to match the news embeddings th clicked histories.<br>\n",
    "\n",
    "We can use the function `combine_user_embedding(user_data, configs)` in load_from_files.py, it will parse the path of `news_embeddings` from configs and then add the news embeddings respectively to the users' tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d5055b9-89af-4bf0-b724-75001aecf26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "combining user with news embeddings.: 100%|██████████| 156965/156965 [00:13<00:00, 11836.54it/s]\n"
     ]
    }
   ],
   "source": [
    "from utils.load_from_files import combine_user_embedding\n",
    "\n",
    "train_user_data = combine_user_embedding(raw_train_user_dataset, stage1_train_config_seperatly_extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3aee0a5b-ff7c-4dc5-ad0f-a4d78eb46f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/16/2024 13:14:46 - [INFO] - UserDataset ->>> successfully built raw user dataset.\n",
      "combining user with news embeddings.: 100%|██████████| 73152/73152 [00:03<00:00, 21681.87it/s]\n"
     ]
    }
   ],
   "source": [
    "# to avoid the memery overflow, the following code will be run seperately to get \n",
    "#user batches dev/test hadamard/lavis.\n",
    "from utils.load_from_files import combine_user_embeddings_test\n",
    "from data_processor.UserDataset import get_UserDataset,UserDataset\n",
    "\n",
    "raw_dev_user_dataset = get_UserDataset(stage1_dev_config_seperatly_extract)\n",
    "dev_user_data = combine_user_embeddings_test(raw_dev_user_dataset, stage1_dev_config_seperatly_extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5c84238-202e-4b99-b7dd-6c4af7e05dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/14/2024 16:22:51 - [INFO] - UserDataset ->>> successfully built raw user dataset.\n",
      "combining user with news embeddings.: 100%|██████████| 2370727/2370727 [02:25<00:00, 16271.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# to avoid the memery overflow, the following code will be run seperately to get \n",
    "#user batches dev/test hadamard/lavis.\n",
    "from utils.load_from_files import combine_user_embeddings_test\n",
    "from data_processor.UserDataset import get_UserDataset,UserDataset\n",
    "\n",
    "raw_test_user_dataset = get_UserDataset(stage1_test_config_seperatly_extract)\n",
    "test_user_data = combine_user_embeddings_test(raw_test_user_dataset, stage1_test_config_seperatly_extract)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9d3b34-9fc9-427d-a51a-2705d88a2fee",
   "metadata": {},
   "source": [
    "after that, we can use the method `load_embedded_user(configs)` at path utils.load_from_files.py to load the embedded user_data from files. Also, the `load_embedded_news(configs)` exists, but not used frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "158fecfc-343f-4da3-852a-e3193b54d269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded train data.\n",
      "loaded dev data.\n"
     ]
    }
   ],
   "source": [
    "from utils.load_from_files import load_embedded_user, load_embedded_news\n",
    "\n",
    "train_user_data = load_embedded_user(stage1_train_config_seperatly_extract)\n",
    "print(\"loaded train data.\" )\n",
    "dev_user_data = load_embedded_user(stage1_dev_config_seperatly_extract)\n",
    "print(\"loaded dev data.\")\n",
    "# test_user_data = load_embedded_user(stage1_test_config_seperatly_extract)\n",
    "# print(\"loaded test data.\")\n",
    "# news_data = load_embedded_news(stage1_config_seperatly_extract)\n",
    "# news_data won't be used in the following code, so it is just a demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872b87a0-83ba-4516-a5b1-6d71341684a1",
   "metadata": {},
   "source": [
    "Then, the user data will be saved at the path correlate to `stage1_config_seperatly_extract.user_path`, and the file will be loaded as a list, the elements are `{\"seq\":……,\"impression\":……,\"target\":0/1}`. Then, we should pad it and build it into a dataset and dataloader.<br>\n",
    "\n",
    "To pad it, I will use the zero padding and the target is 0 as well. To achieve it, can add a padding method at UserDataset or UserDataLoader. In my opinion, it should be at UserDataLoader for it can adapt the max length in batch flexible.<br>\n",
    "First, transfer the user_data into user_dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e05220a1-872d-4f97-a3f1-7898b2d0091b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "768\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from data_processor.UserDataset import get_UserDataset,UserDataset\n",
    "\n",
    "train_user_dataset = UserDataset(train_user_data)\n",
    "print(len(train_user_dataset[126][\"seq\"])) # how long the sequence of histories is.\n",
    "print(len(train_user_dataset[0][\"impression\"])) # the embedding size of impression news.\n",
    "print(train_user_dataset[0]['target']) # the target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a0fcab-ad62-45e3-a4f3-74f875d395e0",
   "metadata": {},
   "source": [
    "Now, let's change it to UserDataloader and use the UserDataLoader. In the class `UserDataLoader`, I added the function `build_batch_fn(self,batch)` and it will be used to pad the sequence and add mask and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1eb5e392-8d30-4aba-8720-2260aecbdd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22826\n",
      "<class 'torch.Tensor'> torch.Size([256, 50, 768])\n",
      "<class 'torch.Tensor'> torch.Size([50])\n",
      "<class 'torch.Tensor'> 256\n"
     ]
    }
   ],
   "source": [
    "from data_processor.UserDataloader import UserDataLoader\n",
    "\n",
    "train_user_dataloader = UserDataLoader(train_user_dataset,stage1_train_config_seperatly_extract)\n",
    "for input_batch in train_user_dataloader:\n",
    "    print(len(train_user_dataloader))\n",
    "    print(type(input_batch[\"batch\"]),input_batch[\"batch\"].shape)\n",
    "    print(type(input_batch[\"mask\"]),input_batch[\"mask\"].shape)\n",
    "    print(type(input_batch[\"target\"]),len(input_batch[\"target\"]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1ad15a2-6063-466b-ba53-b561601da0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([22, 16, 768])\n",
      "<class 'torch.Tensor'> torch.Size([22, 16])\n",
      "<class 'torch.Tensor'> 22\n"
     ]
    }
   ],
   "source": [
    "from data_processor.UserDataset import get_UserDataset,UserDataset\n",
    "from data_processor.TestDataloader import TestDataLoader\n",
    "\n",
    "dev_user_dataset = UserDataset(dev_user_data)\n",
    "dev_user_dataloader = TestDataLoader(dev_user_dataset, stage1_dev_config_seperatly_extract)\n",
    "for input_batch in dev_user_dataloader:\n",
    "    print(type(input_batch[\"batch\"]),input_batch[\"batch\"].shape)\n",
    "    print(type(input_batch[\"mask\"]),input_batch[\"mask\"].shape)\n",
    "    print(type(input_batch[\"target\"]),len(input_batch[\"target\"]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a267b7d4-201c-4f3f-9625-0437b9e700d2",
   "metadata": {},
   "source": [
    "**CODE IN THE NEXT SECTION IGNORED.**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2fe6cb06-cbe5-4b1b-b75a-255bd740eb60",
   "metadata": {},
   "source": [
    "from data_processor.UserDataset import get_UserDataset,UserDataset\n",
    "from data_processor.TestDataloader import TestDataLoader\n",
    "\n",
    "test_user_dataset = UserDataset(test_user_data)\n",
    "test_user_dataloader = TestDataLoader(test_user_dataset, stage1_test_config_seperatly_extract)\n",
    "for input_batch in test_user_dataloader:\n",
    "    print(type(input_batch[\"batch\"]),input_batch[\"batch\"].shape)\n",
    "    print(type(input_batch[\"mask\"]),input_batch[\"mask\"].shape)\n",
    "    print(type(input_batch[\"target\"]),len(input_batch[\"target\"]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aa29e8-85cd-4d35-bed3-203a3b4a0991",
   "metadata": {},
   "source": [
    "The user_dataloader has the format `input_batch = tensor[batchsize, sequence_length, 768]`, `src_mask = tensor[batchsize,sequence_length]`, `target = list[batchsize]`<br>\n",
    "\n",
    "the targets is a list of the user clicked the impression or not which should be predicted in the following works.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefa64c6-4ef0-4fe1-ab9e-bb2daed3d0d6",
   "metadata": {},
   "source": [
    "Now, we have already loaded all of our data and built the batch successfully. Then, we are going to our recommendation system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7953db-1839-4b5c-87b5-89200cf805fb",
   "metadata": {},
   "source": [
    "## 2. Model Structures\n",
    "### 2.1 Recaller\n",
    "\n",
    "Given that we have already obtained the historical sequence embeddings for each user, a straightforward approach is to compute the average of these representations to obtain the user interest embedding. The rationale behind employing such a simple method will be elaborated in section 2.2.3<br>\n",
    "\n",
    "Subsequently, we can embed the newly arrived news and assess their similarity with the user interest. Based on this comparison, we retrieve the top 50 or more relevant news items for further processing in the reranker stage.<br>\n",
    "\n",
    "Since the Recaller component is straightforward and not utilized in this experiment (given that the MIND dataset targets click prediction), we can consider the candidate news as the output of the Recaller. Therefore, the code for this segment will not be provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd703af1-09d4-4ca8-bd55-283619ffa60b",
   "metadata": {},
   "source": [
    "### 2.2 Reranker\n",
    "#### 2.2.1 Model structure\n",
    "In this section, I utilize a Transformer-based **Decoder Only** model to refine the ranking of passages recalled by the recaller<br>\n",
    "\n",
    "From a task perspective, we can conceptualize it as a binary classify task where the translated language comprises two words, '0' and '1'. The predicted probability assigned to '1' represents the rerank score for the respective newly arrived news item.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e747fbfd-6300-4895-88bf-bac5f806b639",
   "metadata": {},
   "source": [
    "#### 2.2.2 Analysis\n",
    "It is imperative to elucidate the rationale behind selecting this particular architecture. Specifically, I will address the following inquiries:<br>\n",
    "\n",
    "1. Why Transformer?\n",
    "2. Why Encoder Only?\n",
    "3. Why employ a special build batch function?<br>\n",
    "\n",
    "Firstly, why Transformer? Traditional RNN-based models lack the capability for parallel computation, thereby compromising computational efficiency. In contrast, Transformer models have demonstrated superior efficiency.<br>\n",
    "\n",
    "Secondly, why Encoder Only? Considering that the Encoder is more good at comprehension than decoder, it is very explicit to use encoder structure in classifier task. However, the encoder might engender a low rank. Inspired by the concept of the  [*Low-Rank Bottleneck in Multi-head Attention Models*](https://arxiv.org/abs/2002.07028), it is observed that an encoder employing multi-head attention might encounter a low-rank bottleneck. But the sequences in our experiments is not as lengthy as typical files and the d_model is large enough to avoid such a problem.<br>\n",
    "\n",
    "Thirdly, why a special build batch function? The rationale lies in preventing the re-ranker from accessing other candidate passages. Analogous to students being discouraged from viewing each other's answers during examinations, such access could potentially lead to inferior performance(you cheated and changed your correct answer.).<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfed3bcd-823a-4963-99c1-d9e14a3ff968",
   "metadata": {},
   "source": [
    "#### 2.2.3 Why do we need Reranker?\n",
    "Initially, the recaller employing a simplistic approach fails to effectively fulfill the task requirements. Evidence from experiments within the realm of Retrieval Augment Generation **[(RAG)](https://zhuanlan.zhihu.com/p/681370855)** indicates that the collaboration between a basic recaller and a reranker yields superior scores .<br>\n",
    "\n",
    "One might question the rationale behind not employing a more sophisticated recaller to attain a more precise outcome before reranking. However, the consideration of computational costs negates this proposition. Opting for a more precise recall, such as using the reranker as the primary recaller, presents a significant computational challenge when confronted with vast volumes of incoming news data, potentially leading to system collapse.<br>\n",
    "\n",
    "In summary, the optimal solution for recommendation entails the combination of a basic recaller with an enhanced reranker. This synergy leverages the strengths of both components, ultimately leading to superior recommendation outcomes. I believe this response adequately addresses the previous inquiry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dde148-a998-4eb1-9814-5a8e3327368a",
   "metadata": {},
   "source": [
    "With the analyses complete, we can proceed to initialize the model for our experiment. Let's move forward with the initialization process.<br>\n",
    "\n",
    "The specifics of initializing the model can be found in `model/RecTransformer.py`. The code has been encapsulated, allowing us to invoke the `build_model` function to obtain a model object initialized using the Xavier method.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd8c5cd2-916e-41c3-a20a-2e577073f3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecTransformer(\n",
      "  (encoder): Encoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x Encoder_Layer(\n",
      "        (self_attention): Multi_Head_Attention(\n",
      "          (linears): ModuleList(\n",
      "            (0-3): 4 x Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (feed_forward): Positionwise_Feed_Forward(\n",
      "          (w_1): Linear(in_features=768, out_features=1024, bias=True)\n",
      "          (w_2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (sublayer): ModuleList(\n",
      "          (0-1): 2 x Sub_Layer_Connection(\n",
      "            (norm): Layer_Norm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): Layer_Norm()\n",
      "  )\n",
      "  (source_embedder): Positional_Embedding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output_layer): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=256, bias=True)\n",
      "    (1): Linear(in_features=256, out_features=16, bias=True)\n",
      "    (2): Linear(in_features=16, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from model.RecTransformer import build_model\n",
    "\n",
    "model = build_model(stage2_train_config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fbf071-eced-4db6-b0bc-ec0d7fe41ca6",
   "metadata": {},
   "source": [
    "Subsequently, equipped with the dataloader and prepared model, we may proceed to utilize the trainer for model training and subsequent evaluation. Detailed information regarding the trainer and evaluator functionalities is available in the files `components/trainer.py` and `components/evaluator.py`, where the classes `Trainer` and `Evaluator` are defined, respectively.<br>\n",
    "\n",
    "However, for convenience, one may simply instantiate the trainer object as follows: `trainer=Trainer(config,model,dataloader,evaluator)`, followed by invoking the `trainer.fit()` method to commence model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b28c5c3-d18b-4af6-92ee-a53fd6d90610",
   "metadata": {},
   "outputs": [],
   "source": [
    "from components.trainer import Trainer\n",
    "from components.evaluator import Evaluator\n",
    "\n",
    "evaluator = Evaluator()\n",
    "\n",
    "trainer = Trainer(stage2_train_config, model, train_user_dataloader,\n",
    "                  dev_user_dataloader, evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa861ea0-418b-4ee2-b4d4-e7686559dba8",
   "metadata": {},
   "source": [
    "After initialize the `trainer`, which wrapped the method including `fit`,`evaluate` and `test`, we can easily call the `trainer.fit()` and `trainer.test()` to train and test the model:<br>\n",
    "\n",
    "train it for an epoch takes about 2 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de375b29-f86e-4aa6-bcff-7c8f62a02307",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/18/2024 17:21:13 - [INFO] - Trainer ->>> start training...\n",
      "train process :   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step:500, loss:1.082\n",
      "global_step:1000, loss:1.589\n",
      "global_step:1500, loss:1.053\n",
      "global_step:2000, loss:1.231\n",
      "global_step:2500, loss:1.364\n",
      "global_step:3000, loss:1.594\n"
     ]
    }
   ],
   "source": [
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7ca916-d4f9-43e1-a353-6d81f14c77d2",
   "metadata": {},
   "source": [
    "Until now, we have finished the one experiment with condition single modal pretrained model. Then, we can train the model with the same method with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef57846-8300-42f5-af5f-ccca9bccf6d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95b64064-6f4e-4858-bc18-56c4a7dcc710",
   "metadata": {},
   "source": [
    "## 3. Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2391ed-017f-4c2e-9ac1-67e08874149e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb47b71-7a3f-4834-8a2f-29703b2cfd47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e31561-c45f-4e61-a4b0-10546b52d20d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fe725d-40f0-4083-887e-00163ada58f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
